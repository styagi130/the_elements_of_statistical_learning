{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "agreed-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import discriminant_analysis, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expanded-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = pathlib.Path(\"./../data/vowel\")\n",
    "\n",
    "train_filepath = data_folder / \"vowel.train\"\n",
    "test_filepath = data_folder / \"vowel.test\"\n",
    "info_filepath = data_folder / \"vowel.info.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lonely-chinese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This info is the original source information for these data.\n",
      "\n",
      "\n",
      "NAME: Vowel Recognition (Deterding data)\n",
      "\n",
      "SUMMARY: Speaker independent recognition of the eleven steady state vowels\n",
      "of British English using a specified training set of lpc derived log area\n",
      "ratios.\n",
      "\n",
      "SOURCE: David Deterding  (data and non-connectionist analysis)\n",
      "        Mahesan Niranjan (first connectionist analysis)\n",
      "        Tony Robinson    (description, program, data, and results)\n",
      "\n",
      "To contact Tony Robinson by electronic mail, use address \n",
      "\"ajr@dsl.eng.cam.ac.uk\"\n",
      "\n",
      "MAINTAINER: neural-bench@cs.cmu.edu\n",
      "\n",
      "PROBLEM DESCRIPTION:\n",
      "\n",
      "The problem is specified by the accompanying data file, \"vowel.data\".  This\n",
      "file is in the standard CMU Neural Network Benchmark format.\n",
      "\n",
      "For a more detailed explanation of the problem, see the excerpt from Tony\n",
      "Robinson's Ph.D. thesis in the COMMENTS section.  In Robinson's opinion,\n",
      "connectionist problems fall into two classes, the possible and the\n",
      "impossible.  He is interested in the latter, by which he means problems\n",
      "that have no exact solution.  Thus the problem here is not to see how fast\n",
      "a network can be trained (although this is important), but to maximise a\n",
      "less than perfect performance.\n",
      "\n",
      "METHODOLOGY:\n",
      "\n",
      "Report the number of test vowels classified correctly, (i.e. the number of\n",
      "occurences when distance of the correct output to the actual output was the\n",
      "smallest of the set of distances from the actual output to all possible\n",
      "target outputs).\n",
      "\n",
      "Though this is not the focus of Robinson's study, it would also be useful\n",
      "to report how long the training took (measured in pattern presentations or\n",
      "with a rough count of floating-point operations required) and what level of\n",
      "success was achieved on the training and testing data after various amounts\n",
      "of training.  Of course, the network topology and algorithm used should be\n",
      "precisely described as well.\n",
      "\n",
      "VARIATIONS:\n",
      "\n",
      "This benchmark is proposed to encourage the exploration of different node\n",
      "types.  Please theorise/experiment/hack.  The author (Robinson) will try to\n",
      "correspond by email if requested.  In particular there has been some\n",
      "discussion recently on the use of a cross-entropy distance measure, and it\n",
      "would be interesting to see results for that.\n",
      "\n",
      "RESULTS:\n",
      "\n",
      "Here is a summary of results obtained by Tony Robinson.  A more complete\n",
      "explanation of this data is given in the exceprt from his thesis in the\n",
      "COMMENTS section below.\n",
      "\n",
      "\n",
      "+-------------------------+--------+---------+---------+\n",
      "|\t\t\t  | no. of | no.     | percent |\n",
      "| Classifier              | hidden | correct | correct |\n",
      "|\t\t\t  | units  |         |         | \n",
      "+-------------------------+--------+---------+---------+\n",
      "| Single-layer perceptron |  -     | 154     | 33      | \n",
      "| Multi-layer perceptron  | 88     | 234     | 51      |\n",
      "| Multi-layer perceptron  | 22     | 206     | 45      |\n",
      "| Multi-layer perceptron  | 11     | 203     | 44      | \n",
      "| Modified Kanerva Model  | 528    | 231     | 50      |\n",
      "| Modified Kanerva Model  | 88     | 197     | 43      | \n",
      "| Radial Basis Function   | 528    | 247     | 53      |\n",
      "| Radial Basis Function   | 88     | 220     | 48      | \n",
      "| Gaussian node network   | 528    | 252     | 55      |\n",
      "| Gaussian node network   | 88     | 247     | 53      |\n",
      "| Gaussian node network   | 22     | 250     | 54      |\n",
      "| Gaussian node network   | 11     | 211     | 47      | \n",
      "| Square node network     | 88     | 253     | 55      |\n",
      "| Square node network     | 22     | 236     | 51      |\n",
      "| Square node network     | 11     | 217     | 50      | \n",
      "| Nearest neighbour       |  -     | 260     | 56      | \n",
      "+-------------------------+--------+---------+---------+\n",
      "\n",
      "Notes: \n",
      "\n",
      "1. Each of these numbers is based on a single trial with random starting\n",
      "weights.  More trials would of course be preferable, but the computational\n",
      "facilities available to Robinson were limited.\n",
      "\n",
      "2. Graphs are given in Robinson's thesis showing test-set performance vs.\n",
      "epoch count for some of the training runs.  In most cases, performance\n",
      "peaks at around 250 correct, after which performance decays to different\n",
      "degrees.  The numbers given above are final performance figures after about\n",
      "3000 trials, not the peak performance obtained during the run.\n",
      "\n",
      "REFERENCES:\n",
      "\n",
      "[Deterding89] D. H. Deterding, 1989, University of Cambridge, \"Speaker\n",
      "\tNormalisation for Automatic Speech Recognition\", submitted for PhD.\n",
      "\n",
      "[NiranjanFallside88] M. Niranjan and F. Fallside, 1988, Cambridge University\n",
      "\tEngineering Department, \"Neural Networks and Radial Basis Functions in\n",
      "\tClassifying Static Speech Patterns\", CUED/F-INFENG/TR.22.\n",
      "\n",
      "[RenalsRohwer89-ijcnn] Steve Renals and Richard Rohwer, \"Phoneme\n",
      "\tClassification Experiments Using Radial Basis Functions\", Submitted to\n",
      "\tthe International Joint Conference on Neural Networks, Washington,\n",
      "\t1989.\n",
      "\n",
      "[RabinerSchafer78] L. R. Rabiner and R. W. Schafer, Englewood Cliffs, New\n",
      "\tJersey, 1978, Prentice Hall, \"Digital Processing of Speech Signals\".\n",
      "\n",
      "[PragerFallside88] R. W. Prager and F. Fallside, 1988, Cambridge University\n",
      "\tEngineering Department, \"The Modified Kanerva Model for Automatic\n",
      "\tSpeech Recognition\", CUED/F-INFENG/TR.6.\n",
      "\n",
      "[BroomheadLowe88] D. Broomhead and D. Lowe, 1988, Royal Signals and Radar\n",
      "\tEstablishment, Malvern, \"Multi-variable Interpolation and Adaptive\n",
      "\tNetworks\", RSRE memo, #4148.\n",
      "\n",
      "[RobinsonNiranjanFallside88-tr] A. J. Robinson and M. Niranjan and F. \n",
      "  \tFallside, 1988, Cambridge University Engineering Department,\n",
      "\t\"Generalising the Nodes of the Error Propagation Network\",\n",
      "\tCUED/F-INFENG/TR.25.\n",
      "\n",
      "[Robinson89] A. J. Robinson, 1989, Cambridge University Engineering\n",
      "\tDepartment, \"Dynamic Error Propagation Networks\".\n",
      "\n",
      "[McCullochAinsworth88] N. McCulloch and W. A. Ainsworth, Proceedings of\n",
      "\tSpeech'88, Edinburgh, 1988, \"Speaker Independent Vowel Recognition\n",
      "\tusing a Multi-Layer Perceptron\".\n",
      "\n",
      "[RobinsonFallside88-neuro] A. J. Robinson and F. Fallside, 1988, Proceedings\n",
      "\tof nEuro'88, Paris, June, \"A Dynamic Connectionist Model for Phoneme\n",
      "\tRecognition.\n",
      "\n",
      "COMMENTS:\n",
      "\n",
      "(By Tony Robinson)\n",
      "\n",
      "The program supplied is slow.  I ran it on several MicroVaxII's for many\n",
      "nights.  I suspect that if I had spent more time on it, it would have been\n",
      "possible to get better results.  Indeed, my later code has a slightly\n",
      "better adaptive step size algotithm, but the old version is given here for\n",
      "comatability with the stated performance values.  It is interesting that,\n",
      "for this problem, the nearest neighbour clasification outperforms any of\n",
      "the connectionist models.  This can be seen as a challange to improve the\n",
      "connectionist performance.\n",
      "\n",
      "The following problem description results and discussion is taken from my\n",
      "PhD thesis.  The aim was to demonstrate that many types of node can be\n",
      "trained using gradient descent.  The full thesis will be available from me\n",
      "when it has been examined, say maybe July 1989.\n",
      "\n",
      "Application to Vowel Recognition\n",
      "--------------------------------\n",
      "\n",
      "This chapter describes the application of a variety of feed-forward networks\n",
      "to the task of recognition of vowel sounds from multiple speakers.  Single\n",
      "speaker vowel recognition studies by Renals and Rohwer [RenalsRohwer89-ijcnn]\n",
      "show that feed-forward networks compare favourably with vector-quantised\n",
      "hidden Markov models.  The vowel data used in this chapter was collected by\n",
      "Deterding [Deterding89], who recorded examples of the eleven steady state\n",
      "vowels of English spoken by fifteen speakers for a speaker normalisation\n",
      "study.  A range of node types are used, as described in the previous section,\n",
      "and some of the problems of the error propagation algorithm are discussed.\n",
      "\n",
      "The Speech Data\n",
      "\n",
      "(An ascii approximation to) the International Phonetic Association (I.P.A.)\n",
      "symbol and the word in which the eleven vowel sounds were recorded is given in\n",
      "table 4.1.  The word was uttered once by each of the fifteen speakers.  Four\n",
      "male and four female speakers were used to train the networks, and the other\n",
      "four male and three female speakers were used for testing the performance.\n",
      "\n",
      "+-------+--------+-------+---------+\n",
      "| vowel |  word  | vowel |  word   | \n",
      "+-------+--------+-------+---------+\n",
      "|  i    |  heed  |  O    |  hod    |\n",
      "|  I    |  hid   |  C:   |  hoard  |\n",
      "|  E    |  head  |  U    |  hood   |\n",
      "|  A    |  had   |  u:   |  who'd  |\n",
      "|  a:   |  hard  |  3:   |  heard  |\n",
      "|  Y    |  hud   |       |         |\n",
      "+-------+--------+-------+---------+\n",
      "\n",
      "Table 4.1: Words used in Recording the Vowels\n",
      "\n",
      "Front End Analysis\n",
      "\n",
      "The speech signals were low pass filtered at 4.7kHz and then digitised to 12\n",
      "bits with a 10kHz sampling rate.  Twelfth order linear predictive analysis was\n",
      "carried out on six 512 sample Hamming windowed segments from the steady part\n",
      "of the vowel.  The reflection coefficients were used to calculate 10 log area\n",
      "parameters, giving a 10 dimensional input space.  For a general introduction\n",
      "to speech processing and an explanation of this technique see Rabiner and\n",
      "Schafer [RabinerSchafer78].\n",
      "\n",
      "Each speaker thus yielded six frames of speech from eleven vowels.  This gave\n",
      "528 frames from the eight speakers used to train the networks and 462 frames\n",
      "from the seven speakers used to test the networks.\n",
      "\n",
      "Details of the Models\n",
      "\n",
      "All the models had common structure of one layer of hidden units and two\n",
      "layers of weights.  Some of the models used fixed weights in the first layer\n",
      "to perform a dimensionality expansion [Robinson89:sect3.1], and the remainder\n",
      "modified the first layer of weights using the error propagation algorithm for\n",
      "general nodes described in [Robinson89:chap2].  In the second layer the hidden\n",
      "units were mapped onto the outputs using the conventional weighted-sum type\n",
      "nodes with a linear activation function.  When Gaussian nodes were used the\n",
      "range of influence of the nodes, w_ij1, was set to the standard deviation of\n",
      "the training data for the appropriate input dimension.  If the locations of\n",
      "these nodes, w_ij0, are placed randomly, then the model behaves like a\n",
      "continuous version of the modified Kanerva model [PragerFallside88].  If the\n",
      "locations are placed at the points defined by the input examples then the\n",
      "model implements a radial basis function [BroomheadLowe88].  The first layer\n",
      "of weights remains constant in both of these models, but can be also trained\n",
      "using the equations of [Robinson89:sect2.4].  Replacing the Gaussian nodes\n",
      "with the conventional type gives a multilayer perceptron and replacing them\n",
      "with conventional nodes with the activation function f(x) = x^2 gives a\n",
      "network of square nodes.  Finally, dispensing with the first layer altogether\n",
      "yields a single layer perceptron.\n",
      "\n",
      "The scaling factor between gradient of the energy and the change made to the\n",
      "weights (the `learning rate', `eta') was dynamically varied during training,\n",
      "as described in [Robinson89:sect2.5].  If the energy decreased this factor was\n",
      "increased by 5%, if it increased the factor was halved.  The networks changed\n",
      "the weights in the direction of steepest descent which is susceptible to\n",
      "finding a local minimum.  A `momentum' term [RumelhartHintonWilliams86] is\n",
      "often used with error propagation networks to smooth the weight changes and\n",
      "`ride over' small local minima.  However, the optimal value of this term is\n",
      "likely to be problem dependent, and in order to provide a uniform framework,\n",
      "this additional term was not used.\n",
      "\n",
      "Recognition Results\n",
      "\n",
      "This experiment was originally carried out with only two frames of data from\n",
      "each word [RobinsonNiranjanFallside88-tr].  In the earlier experiment some\n",
      "problems were encountered with a phenomena termed `overtraining' whereby the\n",
      "recognition rate on the test data peaks part way through training then decays\n",
      "significantly.  The recognition rates for the six frames per word case are\n",
      "given in table 4.2 and are generally higher and show less variability than the\n",
      "previously presented results.  However, the recognition rate on the test set\n",
      "still displays large fluctuations during training, as shown by the plots in\n",
      "[Robinson89:fig3.2] Some fluctuations will arise from the fact that the\n",
      "minimum in weight space for the training set will not be coincident with the\n",
      "minima for the test set.  Thus, half the possible trajectories during learning\n",
      "will approach the test set minimum and then move away from it again on the way\n",
      "to the training set minima [Mark Plumbley, personal communication].  In\n",
      "addition, continued training sharpens the class boundaries which makes the\n",
      "energy insensitive to the class boundary position [Mahesan Niranjan, personal\n",
      "communiation].  For example, there are a large number planes defined with\n",
      "threshold units which will separate two points in the input space, but only\n",
      "one least squares solution for the case of linear units.\n",
      "\n",
      "+-------------------------+--------+---------+---------+\n",
      "|\t\t\t  | no. of | no.     | percent |\n",
      "| Classifier              | hidden | correct | correct |\n",
      "|\t\t\t  | units  |         |         | \n",
      "+-------------------------+--------+---------+---------+\n",
      "| Single-layer perceptron |  -     | 154     | 33      | \n",
      "| Multi-layer perceptron  | 88     | 234     | 51      |\n",
      "| Multi-layer perceptron  | 22     | 206     | 45      |\n",
      "| Multi-layer perceptron  | 11     | 203     | 44      | \n",
      "| Modified Kanerva Model  | 528    | 231     | 50      |\n",
      "| Modified Kanerva Model  | 88     | 197     | 43      | \n",
      "| Radial Basis Function   | 528    | 247     | 53      |\n",
      "| Radial Basis Function   | 88     | 220     | 48      | \n",
      "| Gaussian node network   | 528    | 252     | 55      |\n",
      "| Gaussian node network   | 88     | 247     | 53      |\n",
      "| Gaussian node network   | 22     | 250     | 54      |\n",
      "| Gaussian node network   | 11     | 211     | 47      | \n",
      "| Square node network     | 88     | 253     | 55      |\n",
      "| Square node network     | 22     | 236     | 51      |\n",
      "| Square node network     | 11     | 217     | 50      | \n",
      "| Nearest neighbour       |  -     | 260     | 56      | \n",
      "+-------------------------+--------+---------+---------+\n",
      "\n",
      "Table 4.2: Vowel classification with different non-linear classifiers\n",
      "\n",
      "Discussion\n",
      "\n",
      "From these vowel classification results it can be seen that minimising the\n",
      "least mean square error over a training set does not guarantee good\n",
      "generalisation to the test set.  The best results were achieved with nearest\n",
      "neighbour analysis which classifies an item as the class of the closest\n",
      "example in the training set measured using the Euclidean distance.  It is\n",
      "expected that the problem of overtraining could be overcome by using a larger\n",
      "training set taking data from more speakers.  The performance of the Gaussian\n",
      "and square node network was generally better than that of the multilayer\n",
      "perceptron.  In other speech recognition problems which attempt to classify\n",
      "single frames of speech, such as those described by McCulloch and Ainsworth\n",
      "[McCullochAinsworth88] and that of [Robinson89:chap7 and\n",
      "RobinsonFallside88-neuro], the nearest neighbour algorithm does not perform as\n",
      "well as a multilayer perceptron.  It would be interesting to investigate this\n",
      "difference and apply a network of Gaussian or square nodes to these problems.\n",
      "\n",
      "The initial weights to the hidden units in the Gaussian network can be given a\n",
      "physical interpretation in terms of matching to a template for a set of\n",
      "features.  This gives an advantage both in shortening the training time and\n",
      "also because the network starts at a point in weight space near a likely\n",
      "solution, which avoids some possible local minima which represent poor\n",
      "solutions.\n",
      "\n",
      "The results of the experiments with Gaussian and square nodes are promising.\n",
      "However, it has not been the aim of this chapter to show that a particular\n",
      "type of node is necessarily `better' for error propagation networks than the\n",
      "weighted sum node, but that the error propagation algorithm can be applied\n",
      "successfully to many different types of node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(info_filepath.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-enclosure",
   "metadata": {},
   "source": [
    "# Evaluation metric\n",
    "#### Number of correctly classified vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-klein",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "lovely-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_filepath)\n",
    "test_df = pd.read_csv(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "amino-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row.names      int64\n",
       "y              int64\n",
       "x.1          float64\n",
       "x.2          float64\n",
       "x.3          float64\n",
       "x.4          float64\n",
       "x.5          float64\n",
       "x.6          float64\n",
       "x.7          float64\n",
       "x.8          float64\n",
       "x.9          float64\n",
       "x.10         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "chubby-sudan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>y</th>\n",
       "      <th>x.1</th>\n",
       "      <th>x.2</th>\n",
       "      <th>x.3</th>\n",
       "      <th>x.4</th>\n",
       "      <th>x.5</th>\n",
       "      <th>x.6</th>\n",
       "      <th>x.7</th>\n",
       "      <th>x.8</th>\n",
       "      <th>x.9</th>\n",
       "      <th>x.10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>528.00000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>264.50000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.166695</td>\n",
       "      <td>1.735343</td>\n",
       "      <td>-0.448002</td>\n",
       "      <td>0.524983</td>\n",
       "      <td>-0.389280</td>\n",
       "      <td>0.584960</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.417394</td>\n",
       "      <td>-0.268112</td>\n",
       "      <td>-0.084568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>152.56474</td>\n",
       "      <td>3.165277</td>\n",
       "      <td>0.957965</td>\n",
       "      <td>1.160970</td>\n",
       "      <td>0.741363</td>\n",
       "      <td>0.769361</td>\n",
       "      <td>0.722011</td>\n",
       "      <td>0.648547</td>\n",
       "      <td>0.479254</td>\n",
       "      <td>0.595580</td>\n",
       "      <td>0.619584</td>\n",
       "      <td>0.560317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.211000</td>\n",
       "      <td>-1.274000</td>\n",
       "      <td>-2.487000</td>\n",
       "      <td>-1.409000</td>\n",
       "      <td>-2.127000</td>\n",
       "      <td>-0.836000</td>\n",
       "      <td>-1.537000</td>\n",
       "      <td>-1.293000</td>\n",
       "      <td>-1.613000</td>\n",
       "      <td>-1.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>132.75000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-3.923000</td>\n",
       "      <td>0.916750</td>\n",
       "      <td>-0.945500</td>\n",
       "      <td>-0.083500</td>\n",
       "      <td>-0.930750</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>-0.297000</td>\n",
       "      <td>-0.018250</td>\n",
       "      <td>-0.673750</td>\n",
       "      <td>-0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>264.50000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.097000</td>\n",
       "      <td>1.733000</td>\n",
       "      <td>-0.502500</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>-0.417000</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>-0.255000</td>\n",
       "      <td>-0.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>396.25000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-2.511750</td>\n",
       "      <td>2.403750</td>\n",
       "      <td>0.049250</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>1.009750</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.861250</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>528.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-0.941000</td>\n",
       "      <td>5.074000</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>2.191000</td>\n",
       "      <td>1.831000</td>\n",
       "      <td>2.327000</td>\n",
       "      <td>1.403000</td>\n",
       "      <td>1.673000</td>\n",
       "      <td>1.309000</td>\n",
       "      <td>1.396000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       row.names           y         x.1         x.2         x.3         x.4  \\\n",
       "count  528.00000  528.000000  528.000000  528.000000  528.000000  528.000000   \n",
       "mean   264.50000    6.000000   -3.166695    1.735343   -0.448002    0.524983   \n",
       "std    152.56474    3.165277    0.957965    1.160970    0.741363    0.769361   \n",
       "min      1.00000    1.000000   -5.211000   -1.274000   -2.487000   -1.409000   \n",
       "25%    132.75000    3.000000   -3.923000    0.916750   -0.945500   -0.083500   \n",
       "50%    264.50000    6.000000   -3.097000    1.733000   -0.502500    0.456500   \n",
       "75%    396.25000    9.000000   -2.511750    2.403750    0.049250    1.164000   \n",
       "max    528.00000   11.000000   -0.941000    5.074000    1.413000    2.191000   \n",
       "\n",
       "              x.5         x.6         x.7         x.8         x.9        x.10  \n",
       "count  528.000000  528.000000  528.000000  528.000000  528.000000  528.000000  \n",
       "mean    -0.389280    0.584960    0.017477    0.417394   -0.268112   -0.084568  \n",
       "std      0.722011    0.648547    0.479254    0.595580    0.619584    0.560317  \n",
       "min     -2.127000   -0.836000   -1.537000   -1.293000   -1.613000   -1.680000  \n",
       "25%     -0.930750    0.108500   -0.297000   -0.018250   -0.673750   -0.507000  \n",
       "50%     -0.417000    0.527500    0.040000    0.477000   -0.255000   -0.082500  \n",
       "75%      0.115500    1.009750    0.348000    0.861250    0.137500    0.301000  \n",
       "max      1.831000    2.327000    1.403000    1.673000    1.309000    1.396000  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "functional-projection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>y</th>\n",
       "      <th>x.1</th>\n",
       "      <th>x.2</th>\n",
       "      <th>x.3</th>\n",
       "      <th>x.4</th>\n",
       "      <th>x.5</th>\n",
       "      <th>x.6</th>\n",
       "      <th>x.7</th>\n",
       "      <th>x.8</th>\n",
       "      <th>x.9</th>\n",
       "      <th>x.10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>231.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.246078</td>\n",
       "      <td>2.049102</td>\n",
       "      <td>-0.576076</td>\n",
       "      <td>0.504626</td>\n",
       "      <td>-0.210089</td>\n",
       "      <td>0.681998</td>\n",
       "      <td>-0.029327</td>\n",
       "      <td>0.244162</td>\n",
       "      <td>-0.342820</td>\n",
       "      <td>-0.056221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>133.512172</td>\n",
       "      <td>3.165706</td>\n",
       "      <td>0.753377</td>\n",
       "      <td>1.170402</td>\n",
       "      <td>0.671069</td>\n",
       "      <td>0.748236</td>\n",
       "      <td>0.578353</td>\n",
       "      <td>0.544476</td>\n",
       "      <td>0.440483</td>\n",
       "      <td>0.532523</td>\n",
       "      <td>0.505557</td>\n",
       "      <td>0.650602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.982000</td>\n",
       "      <td>-1.074000</td>\n",
       "      <td>-2.091000</td>\n",
       "      <td>-1.044000</td>\n",
       "      <td>-1.733000</td>\n",
       "      <td>-0.405000</td>\n",
       "      <td>-1.282000</td>\n",
       "      <td>-0.949000</td>\n",
       "      <td>-1.409000</td>\n",
       "      <td>-1.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>116.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-3.855750</td>\n",
       "      <td>1.194000</td>\n",
       "      <td>-1.037000</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>-0.612000</td>\n",
       "      <td>0.278250</td>\n",
       "      <td>-0.310250</td>\n",
       "      <td>-0.167750</td>\n",
       "      <td>-0.721750</td>\n",
       "      <td>-0.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>231.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>2.101500</td>\n",
       "      <td>-0.621000</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>-0.181500</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>-0.358000</td>\n",
       "      <td>-0.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>346.750000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-2.706500</td>\n",
       "      <td>2.985000</td>\n",
       "      <td>-0.181000</td>\n",
       "      <td>0.960750</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>1.038500</td>\n",
       "      <td>0.245750</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.594750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>462.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-1.093000</td>\n",
       "      <td>4.314000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>2.377000</td>\n",
       "      <td>1.114000</td>\n",
       "      <td>2.108000</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>2.039000</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.294000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        row.names           y         x.1         x.2         x.3         x.4  \\\n",
       "count  462.000000  462.000000  462.000000  462.000000  462.000000  462.000000   \n",
       "mean   231.500000    6.000000   -3.246078    2.049102   -0.576076    0.504626   \n",
       "std    133.512172    3.165706    0.753377    1.170402    0.671069    0.748236   \n",
       "min      1.000000    1.000000   -4.982000   -1.074000   -2.091000   -1.044000   \n",
       "25%    116.250000    3.000000   -3.855750    1.194000   -1.037000   -0.049250   \n",
       "50%    231.500000    6.000000   -3.220000    2.101500   -0.621000    0.418500   \n",
       "75%    346.750000    9.000000   -2.706500    2.985000   -0.181000    0.960750   \n",
       "max    462.000000   11.000000   -1.093000    4.314000    1.431000    2.377000   \n",
       "\n",
       "              x.5         x.6         x.7         x.8         x.9        x.10  \n",
       "count  462.000000  462.000000  462.000000  462.000000  462.000000  462.000000  \n",
       "mean    -0.210089    0.681998   -0.029327    0.244162   -0.342820   -0.056221  \n",
       "std      0.578353    0.544476    0.440483    0.532523    0.505557    0.650602  \n",
       "min     -1.733000   -0.405000   -1.282000   -0.949000   -1.409000   -1.241000  \n",
       "25%     -0.612000    0.278250   -0.310250   -0.167750   -0.721750   -0.564500  \n",
       "50%     -0.181500    0.593000    0.005500    0.245000   -0.358000   -0.257500  \n",
       "75%      0.199000    1.038500    0.245750    0.651500    0.019500    0.594750  \n",
       "max      1.114000    2.108000    1.209000    2.039000    0.757000    1.294000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-carrier",
   "metadata": {},
   "source": [
    "## Fit the discriminant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-norwegian",
   "metadata": {},
   "source": [
    "#### Preprocess the data calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "substantial-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors = train_df.drop(\"row.names\", axis=1).value_counts(\"y\")\n",
    "class_priors = (class_priors/class_priors.sum()).values\n",
    "\n",
    "X_train, y_train = train_df.drop([\"row.names\", \"y\"], axis=1).values, train_df.loc[:, \"y\"].values\n",
    "X_test, y_test = test_df.drop([\"row.names\", \"y\"], axis=1).values, test_df.loc[:, \"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "offshore-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample size: 528, Train predictors: 10\n",
      "Test sample size: 462, Test predictors: 10\n"
     ]
    }
   ],
   "source": [
    "print( f\"Train sample size: {X_train.shape[0]}, Train predictors: {X_train.shape[1]}\")\n",
    "print( f\"Test sample size: {X_test.shape[0]}, Test predictors: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "expired-michael",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=array([0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909, 0.09090909, 0.09090909, 0.09090909, 0.09090909,\n",
       "       0.09090909]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = discriminant_analysis.QuadraticDiscriminantAnalysis(priors = class_priors)\n",
    "qda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-billion",
   "metadata": {},
   "source": [
    "**For train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "spread-easter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for QDA is: 0.9886363636363636\n"
     ]
    }
   ],
   "source": [
    "y_pred = qda.predict(X_train)\n",
    "train_acc = (y_pred == y_train).sum()/y_pred.shape[0]\n",
    "print(f\"Training accuracy for QDA is: {train_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-costs",
   "metadata": {},
   "source": [
    "**For test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "oriental-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = qda.predict(X_test)\n",
    "test_acc = (y_pred == y_test).sum()/y_pred.shape(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-apartment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8 basic_ml",
   "language": "python",
   "name": "basic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
